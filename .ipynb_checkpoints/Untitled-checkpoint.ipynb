{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc5555c7-f5bf-4c10-9f94-55cbb1aa5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.ops import roi_align\n",
    "import numpy as np\n",
    "import os\n",
    "from got10k.trackers import Tracker\n",
    "from got10k.datasets import GOT10k, UAV123\n",
    "from got10k.utils.viz import show_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b89e254a-92c2-4cbc-a81d-32b82ec72ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "EPOCHS = 10\n",
    "W = 512\n",
    "H = 512\n",
    "LAMBDA = 1\n",
    "BATCHSIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc964443-b978-4fd4-b01c-bebef90500cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor\n",
    "class ResnetFeature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetFeature, self).__init__()\n",
    "        self.feat = {}\n",
    "        def get_features(features, name):\n",
    "            def hook(model, input, output):\n",
    "                features[name] = output.detach()\n",
    "            return hook\n",
    "        self.f = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.f.layer2.register_forward_hook(get_features(self.feat, 'feat1'))\n",
    "        self.f.layer3.register_forward_hook(get_features(self.feat, 'feat2'))\n",
    "        self.f.layer4.register_forward_hook(get_features(self.feat, 'feat3'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        o = self.f(x)\n",
    "        return self.feat['feat1'], self.feat['feat2'], self.feat['feat3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "201eb527-f464-4adf-9cd7-ba03d34f88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullNet, self).__init__()\n",
    "        self.feature_net = ResnetFeature()\n",
    "    \n",
    "    def forward(self, query_image, target_box, search_image):\n",
    "        # qf = query features, sf = search features\n",
    "        qf1, qf2, qf3 = self.feature_net(query_image)\n",
    "        sf1, sf2, sf3 = self.feature_net(search_image)\n",
    "        \n",
    "        roi11 = roi_align(input=qf1, boxes=target_box, output_size=3, spatial_scale=qf1.shape[-1]/W, aligned=True)\n",
    "        roi12 = roi_align(input=qf1, boxes=target_box, output_size=5, spatial_scale=qf1.shape[-1]/W, aligned=True)\n",
    "        roi13 = roi_align(input=qf1, boxes=target_box, output_size=7, spatial_scale=qf1.shape[-1]/W, aligned=True)\n",
    "        \n",
    "        roi21 = roi_align(input=qf2, boxes=target_box, output_size=3, spatial_scale=qf2.shape[-1]/W, aligned=True)\n",
    "        roi22 = roi_align(input=qf2, boxes=target_box, output_size=5, spatial_scale=qf2.shape[-1]/W, aligned=True)\n",
    "        roi23 = roi_align(input=qf2, boxes=target_box, output_size=7, spatial_scale=qf2.shape[-1]/W, aligned=True)\n",
    "        \n",
    "        roi31 = roi_align(input=qf3, boxes=target_box, output_size=3, spatial_scale=qf3.shape[-1]/W, aligned=True)\n",
    "        roi32 = roi_align(input=qf3, boxes=target_box, output_size=5, spatial_scale=qf3.shape[-1]/W, aligned=True)\n",
    "        roi33 = roi_align(input=qf3, boxes=target_box, output_size=7, spatial_scale=qf3.shape[-1]/W, aligned=True)\n",
    "        \n",
    "        #return cls_output, reg_output\n",
    "        return roi11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610070dd-ed89-4051-947b-2266cec6f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement tracker to conduct got10k experiments\n",
    "\n",
    "#resize image and box, TODO\n",
    "def prepro(image, box=None):\n",
    "    return image, box\n",
    "\n",
    "#get best box from classifier and regressor outputs, TODO\n",
    "def find_box(cls_output, reg_output):\n",
    "    return found_box\n",
    "\n",
    "#get training labels from ground truth anno, TODO\n",
    "def anno_to_labels(anno):\n",
    "    return cls_label, reg_label\n",
    "\n",
    "#TODO\n",
    "def read_image(image_file):\n",
    "    return image\n",
    "\n",
    "\n",
    "class TrackerA(Tracker):\n",
    "    def __init__(self):\n",
    "        super(TrackerA, self).__init__()\n",
    "        self.net = FullNet()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=LR)\n",
    "        \n",
    "    \n",
    "    #this one we use for our training\n",
    "    def step(self, image_files, annos):\n",
    "        self.qs = []\n",
    "        self.ss = []\n",
    "        self.bs = []\n",
    "        for i in range(annos.shape[0]):\n",
    "            image_file = image_files[i]\n",
    "            image = read_image(image_file)\n",
    "            anno = annos[i]\n",
    "            \n",
    "            #if first frame\n",
    "            if i == 0:\n",
    "                self.query_image, self.target_box = prepro(image, anno)\n",
    "                continue\n",
    "            \n",
    "            self.search_image = prepro(image)\n",
    "            \n",
    "            self.qs.append(self.query_image)\n",
    "            self.ss.append(self.search_image)\n",
    "            self.bs.append(self.target_box)\n",
    "            \n",
    "            # train when batch\n",
    "            if i % BATCHSIZE == 0:\n",
    "                cls_output, reg_output = self.net(torch.cat(self.qs, 0), self.bs, torch.cat(self.ss, 0))\n",
    "            \n",
    "                #getting losses\n",
    "                cls_label, reg_label = anno_to_labels(self.bs)\n",
    "                cls_loss, reg_loss = get_losses(cls_output, reg_output, cls_label, reg_label)\n",
    "                loss = cls_loss + LAMBDA * reg_loss\n",
    "            \n",
    "                #training\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                #reset batch lists\n",
    "                self.qs = []\n",
    "                self.ss = []\n",
    "                self.bs = []\n",
    "    \n",
    "    #save model while training\n",
    "    def save(self, epoch)\n",
    "        path = os.path.join(\"./modelsave/\", \"model_\" + str(epoch) + \".pth\")\n",
    "    \n",
    "    #these two are used in experiments, TODO\n",
    "    def init(self, image, box):\n",
    "        self.query_image, self.target_box = prepro(image, box)\n",
    "    \n",
    "    def update(self, image):\n",
    "        self.search_image = prepro(image)\n",
    "        cls_output, reg_output = self.net(self.query_image, self.target_box, self.search_image)\n",
    "        found_box = find_box(cls_output, reg_output)\n",
    "        return found_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7620c05-6703-4837-8cc2-04daf2be1a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4a189c2-33f5-4fe9-b085-2b033c3a64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FullNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7b75d72-26b4-4afd-9aa2-f1077b6334e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(8, 3, 512, 512)\n",
    "s = torch.randn(8, 3, 512, 512)\n",
    "b = [torch.randn(1, 4) for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c399f5a-9991-4a42-a19a-a8ca170ed9d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36mFullNet.forward\u001b[0;34m(self, query_image, target_box, search_image)\u001b[0m\n\u001b[1;32m      8\u001b[0m qf1, qf2, qf3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_net(query_image)\n\u001b[1;32m      9\u001b[0m sf1, sf2, sf3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_net(search_image)\n\u001b[0;32m---> 11\u001b[0m roi11 \u001b[38;5;241m=\u001b[39m \u001b[43mroi_align\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m roi12 \u001b[38;5;241m=\u001b[39m roi_align(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mqf1, boxes\u001b[38;5;241m=\u001b[39mtarget_box, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, spatial_scale\u001b[38;5;241m=\u001b[39mqf1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39mW, aligned\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m roi13 \u001b[38;5;241m=\u001b[39m roi_align(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mqf1, boxes\u001b[38;5;241m=\u001b[39mtarget_box, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, spatial_scale\u001b[38;5;241m=\u001b[39mqf1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39mW, aligned\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torchvision/ops/roi_align.py:55\u001b[0m, in \u001b[0;36mroi_align\u001b[0;34m(input, boxes, output_size, spatial_scale, sampling_ratio, aligned)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m     54\u001b[0m     _log_api_usage_once(roi_align)\n\u001b[0;32m---> 55\u001b[0m \u001b[43m_assert_has_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m check_roi_boxes_shape(boxes)\n\u001b[1;32m     57\u001b[0m rois \u001b[38;5;241m=\u001b[39m boxes\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torchvision/extension.py:33\u001b[0m, in \u001b[0;36m_assert_has_ops\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_has_ops\u001b[39m():\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_ops():\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load custom C++ ops. This can happen if your PyTorch and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision versions are incompatible, or if you had errors while compiling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision from source. For further information on the compatible versions, check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check your PyTorch version with torch.__version__ and your torchvision \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion with torchvision.__version__ and verify if they are compatible, and if not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease reinstall torchvision so that it matches your PyTorch install.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "f(q, b, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d416738-5159-4eb0-965d-faff44857dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87702794-311b-4203-afb4-0243f3b7e2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3804d-8721-4deb-b42f-222337e951e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d37c2-c416-459f-9e75-9bf0cf60c75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2c51e-6783-4e6e-8f8e-2c1bcaa2069f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85668f56-1405-49c6-abe9-afef2a23e5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae07bb1-8563-4fec-b756-c7025d57fb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17aa43-cf1a-49bc-8f9e-f31e36c89820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bc8fb-3244-4f28-ac61-d1ae4dbd432f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556728b-8bf7-4fa3-986b-c7681ec31d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UAV123(root_dir='./data/UAV123/')\n",
    "#tracker = TrackerA()\n",
    "for epoch in range(EPOCHS):\n",
    "    for image_files, annos in dataset:\n",
    "        #tracker.step(image_files, annos)\n",
    "        print(annos[0])\n",
    "    tracker.save(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4f34c-568c-441c-b142-f01e8e77f7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
