{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5555c7-f5bf-4c10-9f94-55cbb1aa5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import os\n",
    "from got10k.trackers import Tracker\n",
    "from got10k.datasets import DTB70, NfS, OTB, TColor128, TrackingNet, UAV123, VOT\n",
    "from got10k.utils.viz import show_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e254a-92c2-4cbc-a81d-32b82ec72ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "EPOCHS = 10\n",
    "W = 512\n",
    "H = 512\n",
    "LAMBDA = 1\n",
    "BATCHSIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc964443-b978-4fd4-b01c-bebef90500cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor\n",
    "class ResnetFeature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetFeature, self).__init__()\n",
    "        self.feat = {}\n",
    "        def get_features(features, name):\n",
    "            def hook(model, input, output):\n",
    "                features[name] = output.detach()\n",
    "            return hook\n",
    "        self.f = resnet50(pretrained=True)\n",
    "        self.f.layer2.register_forward_hook(get_features(self.feat, 'feat1'))\n",
    "        self.f.layer3.register_forward_hook(get_features(self.feat, 'feat2'))\n",
    "        self.f.layer4.register_forward_hook(get_features(self.feat, 'feat3'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        o = self.f(x)\n",
    "        return self.feat['feat1'], self.feat['feat2'], self.feat['feat3']\n",
    "\n",
    "# TODO\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetFeature, self).__init__()\n",
    "        \n",
    "    def forward(self, scoremap1, scoremap2, scoremap3):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201eb527-f464-4adf-9cd7-ba03d34f88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullNet, self).__init__()\n",
    "        self.feature_net = ResnetFeature()\n",
    "        self.rpn = RPN()\n",
    "    \n",
    "    def forward(self, query_image, target_box, search_image):\n",
    "        # get features\n",
    "        # qf = query features, sf = search features\n",
    "        qf1, qf2, qf3 = self.feature_net(query_image)\n",
    "        sf1, sf2, sf3 = self.feature_net(search_image)\n",
    "        \n",
    "        #get roi features from query\n",
    "        roi11 = roi_align(input=qf1, boxes=target_box, output_size=3, spatial_scale=qf1.shape[-1]/W, aligned=True)\n",
    "        roi12 = roi_align(input=qf1, boxes=target_box, output_size=5, spatial_scale=qf1.shape[-1]/W, aligned=True)\n",
    "        roi13 = roi_align(input=qf1, boxes=target_box, output_size=7, spatial_scale=qf1.shape[-1]/W, aligned=True)\n",
    "        \n",
    "        roi21 = roi_align(input=qf2, boxes=target_box, output_size=3, spatial_scale=qf2.shape[-1]/W, aligned=True)\n",
    "        roi22 = roi_align(input=qf2, boxes=target_box, output_size=5, spatial_scale=qf2.shape[-1]/W, aligned=True)\n",
    "        roi23 = roi_align(input=qf2, boxes=target_box, output_size=7, spatial_scale=qf2.shape[-1]/W, aligned=True)\n",
    "        \n",
    "        roi31 = roi_align(input=qf3, boxes=target_box, output_size=3, spatial_scale=qf3.shape[-1]/W, aligned=True)\n",
    "        roi32 = roi_align(input=qf3, boxes=target_box, output_size=5, spatial_scale=qf3.shape[-1]/W, aligned=True)\n",
    "        roi33 = roi_align(input=qf3, boxes=target_box, output_size=7, spatial_scale=qf3.shape[-1]/W, aligned=True)\n",
    "        \n",
    "        # cross corr templates with search features groupwise, each output has 16 channels\n",
    "        scoremap11 = F.conv2d(input=sf1.reshape(1, -1, sf1.shape[-2], sf1.shape[-1]), weight=roi11.reshape(roi11.shape[0]*16, roi11.shape[1]//16, roi11.shape[2], roi11.shape[3]), groups=16*sf1.shape[0], padding=0)\n",
    "        scoremap12 = F.conv2d(input=sf1.reshape(1, -1, sf1.shape[-2], sf1.shape[-1]), weight=roi12.reshape(roi12.shape[0]*16, roi12.shape[1]//16, roi12.shape[2], roi12.shape[3]), groups=16*sf1.shape[0], padding=1)\n",
    "        scoremap13 = F.conv2d(input=sf1.reshape(1, -1, sf1.shape[-2], sf1.shape[-1]), weight=roi13.reshape(roi13.shape[0]*16, roi13.shape[1]//16, roi13.shape[2], roi13.shape[3]), groups=16*sf1.shape[0], padding=2)\n",
    "        scoremap11 = scoremap11.reshape(sf1.shape[0], -1, scoremap11.shape[2], scoremap11.shape[3])\n",
    "        scoremap12 = scoremap12.reshape(sf1.shape[0], -1, scoremap12.shape[2], scoremap12.shape[3])\n",
    "        scoremap13 = scoremap13.reshape(sf1.shape[0], -1, scoremap13.shape[2], scoremap13.shape[3])\n",
    "        scoremap1 = torch.cat([scoremap11, scoremap12, scoremap13], 1)\n",
    "        \n",
    "        scoremap21 = F.conv2d(input=sf2.reshape(1, -1, sf2.shape[-2], sf2.shape[-1]), weight=roi21.reshape(roi21.shape[0]*16, roi21.shape[1]//16, roi21.shape[2], roi21.shape[3]), groups=16*sf2.shape[0], padding=0)\n",
    "        scoremap22 = F.conv2d(input=sf2.reshape(1, -1, sf2.shape[-2], sf2.shape[-1]), weight=roi22.reshape(roi22.shape[0]*16, roi22.shape[1]//16, roi22.shape[2], roi22.shape[3]), groups=16*sf2.shape[0], padding=1)\n",
    "        scoremap23 = F.conv2d(input=sf2.reshape(1, -1, sf2.shape[-2], sf2.shape[-1]), weight=roi23.reshape(roi23.shape[0]*16, roi23.shape[1]//16, roi23.shape[2], roi23.shape[3]), groups=16*sf2.shape[0], padding=2)\n",
    "        scoremap21 = scoremap21.reshape(sf2.shape[0], -1, scoremap21.shape[2], scoremap21.shape[3])\n",
    "        scoremap22 = scoremap22.reshape(sf2.shape[0], -1, scoremap22.shape[2], scoremap22.shape[3])\n",
    "        scoremap23 = scoremap23.reshape(sf2.shape[0], -1, scoremap23.shape[2], scoremap23.shape[3])\n",
    "        scoremap2 = torch.cat([scoremap21, scoremap22, scoremap23], 1)\n",
    "        \n",
    "        scoremap31 = F.conv2d(input=sf3.reshape(1, -1, sf3.shape[-2], sf3.shape[-1]), weight=roi31.reshape(roi31.shape[0]*16, roi31.shape[1]//16, roi31.shape[2], roi31.shape[3]), groups=16*sf3.shape[0], padding=0)\n",
    "        scoremap32 = F.conv2d(input=sf3.reshape(1, -1, sf3.shape[-2], sf3.shape[-1]), weight=roi32.reshape(roi32.shape[0]*16, roi32.shape[1]//16, roi32.shape[2], roi32.shape[3]), groups=16*sf3.shape[0], padding=1)\n",
    "        scoremap33 = F.conv2d(input=sf3.reshape(1, -1, sf3.shape[-2], sf3.shape[-1]), weight=roi33.reshape(roi33.shape[0]*16, roi33.shape[1]//16, roi33.shape[2], roi33.shape[3]), groups=16*sf3.shape[0], padding=2)\n",
    "        scoremap31 = scoremap31.reshape(sf3.shape[0], -1, scoremap31.shape[2], scoremap31.shape[3])\n",
    "        scoremap32 = scoremap32.reshape(sf3.shape[0], -1, scoremap32.shape[2], scoremap32.shape[3])\n",
    "        scoremap33 = scoremap33.reshape(sf3.shape[0], -1, scoremap33.shape[2], scoremap33.shape[3])\n",
    "        scoremap3 = torch.cat([scoremap31, scoremap32, scoremap33], 1)\n",
    "        \n",
    "        # region proposal\n",
    "        cls_output, reg_output = self.rpn(scoremap1, scoremap2, scoremap3)\n",
    "        \n",
    "        return cls_output, reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610070dd-ed89-4051-947b-2266cec6f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement tracker to conduct got10k experiments\n",
    "\n",
    "#resize image and box, TODO\n",
    "def prepro(image, box=None):\n",
    "    return image, box\n",
    "\n",
    "#get best box from classifier and regressor outputs, TODO\n",
    "def find_box(cls_output, reg_output):\n",
    "    return found_box\n",
    "\n",
    "#get training labels from ground truth anno, TODO\n",
    "def anno_to_labels(anno):\n",
    "    return cls_label, reg_label\n",
    "\n",
    "#TODO\n",
    "def readimage(image_file):\n",
    "    image = read_image(image_file).astype(torch.float32)\n",
    "    return image\n",
    "\n",
    "\n",
    "class TrackerA(Tracker):\n",
    "    def __init__(self):\n",
    "        super(TrackerA, self).__init__()\n",
    "        self.net = FullNet()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=LR)\n",
    "        \n",
    "    \n",
    "    #this one we use for our training\n",
    "    def step(self, image_files, annos):\n",
    "        self.qs = []\n",
    "        self.ss = []\n",
    "        self.bs = []\n",
    "        for i in range(annos.shape[0]):\n",
    "            image_file = image_files[i]\n",
    "            image = readimage(image_file)\n",
    "            anno = annos[i]\n",
    "            \n",
    "            #if first frame\n",
    "            if i == 0:\n",
    "                self.query_image, self.target_box = prepro(image, anno)\n",
    "                continue\n",
    "            \n",
    "            self.search_image = prepro(image)\n",
    "            \n",
    "            self.qs.append(self.query_image)\n",
    "            self.ss.append(self.search_image)\n",
    "            self.bs.append(self.target_box)\n",
    "            \n",
    "            # train when batch\n",
    "            if i % BATCHSIZE == 0:\n",
    "                cls_output, reg_output = self.net(torch.cat(self.qs, 0), self.bs, torch.cat(self.ss, 0))\n",
    "            \n",
    "                #getting losses\n",
    "                cls_label, reg_label = anno_to_labels(self.bs)\n",
    "                cls_loss, reg_loss = get_losses(cls_output, reg_output, cls_label, reg_label)\n",
    "                loss = cls_loss + LAMBDA * reg_loss\n",
    "            \n",
    "                #training\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                #reset batch lists\n",
    "                self.qs = []\n",
    "                self.ss = []\n",
    "                self.bs = []\n",
    "    \n",
    "    #save model while training\n",
    "    def save(self, epoch)\n",
    "        path = os.path.join(\"./modelsave/\", \"model_\" + str(epoch) + \".pth\")\n",
    "    \n",
    "    #these two are used in experiments, TODO\n",
    "    def init(self, image, box):\n",
    "        self.query_image, self.target_box = prepro(image, box)\n",
    "    \n",
    "    def update(self, image):\n",
    "        self.search_image = prepro(image)\n",
    "        cls_output, reg_output = self.net(self.query_image, self.target_box, self.search_image)\n",
    "        found_box = find_box(cls_output, reg_output)\n",
    "        return found_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7620c05-6703-4837-8cc2-04daf2be1a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a189c2-33f5-4fe9-b085-2b033c3a64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FullNet()\n",
    "q = torch.randn(BATCHSIZE, 3, 512, 512)\n",
    "s = torch.randn(BATCHSIZE, 3, 512, 512)\n",
    "b = [torch.randn(1, 4) for i in range(BATCHSIZE)]\n",
    "o = f(q,b,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d37c2-c416-459f-9e75-9bf0cf60c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for oo in o:\n",
    "    print(oo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2c51e-6783-4e6e-8f8e-2c1bcaa2069f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85668f56-1405-49c6-abe9-afef2a23e5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae07bb1-8563-4fec-b756-c7025d57fb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17aa43-cf1a-49bc-8f9e-f31e36c89820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bc8fb-3244-4f28-ac61-d1ae4dbd432f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556728b-8bf7-4fa3-986b-c7681ec31d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UAV123(root_dir='./data/UAV123/')\n",
    "#tracker = TrackerA()\n",
    "for epoch in range(EPOCHS):\n",
    "    for image_files, annos in dataset:\n",
    "        #tracker.step(image_files, annos)\n",
    "        print(annos[0])\n",
    "    # tracker.save(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4f34c-568c-441c-b142-f01e8e77f7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9cfbb-0e8a-40cc-be0e-075471a7abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = OTB(root_dir='data/OTB', download=True, version='tb50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b36f2-c73c-4b28-8389-4157fa3bb787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
